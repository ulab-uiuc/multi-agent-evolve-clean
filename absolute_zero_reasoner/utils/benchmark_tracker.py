"""
Benchmark tracking and analysis system for monitoring test performance over time.
"""

import json
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from collections import defaultdict
import numpy as np
from datetime import datetime

from absolute_zero_reasoner.utils.logging_utils.stdout import PrettyPrinter


@dataclass
class BenchmarkResult:
    """Single benchmark result for a specific question."""
    question_id: str
    question: str
    model_answer: str
    correct_answer: str
    is_correct: bool
    score: float
    step: int
    timestamp: str
    benchmark_name: str


@dataclass
class StepSummary:
    """Summary of all benchmarks at a specific step."""
    step: int
    timestamp: str
    overall_accuracy: float
    benchmark_accuracies: Dict[str, float]
    total_questions: int
    correct_questions: int


class BenchmarkTracker:
    """Tracks benchmark performance over time and generates improvement suggestions."""
    
    def __init__(self, output_dir: str = "./benchmark_tracking", config=None):
        """Initialize benchmark tracker with persistent storage."""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"[DEBUG] BenchmarkTracker.__init__: output_dir={self.output_dir}")
        print(f"[DEBUG] BenchmarkTracker.__init__: config type={type(config)}")
        
        # Files for persistent storage
        self.history_file = self.output_dir / "benchmark_history.pkl"
        self.summaries_file = self.output_dir / "step_summaries.pkl"
        
        # In-memory data structures
        self.benchmark_history: List[BenchmarkResult] = []
        self.step_summaries: List[StepSummary] = []
        self._question_history: Dict[str, List[BenchmarkResult]] = defaultdict(list)
        
        # Load existing data if available
        self.benchmark_history = self._load_history()
        self.step_summaries = self._load_summaries()
        self._rebuild_question_cache()
        
        print(f"[DEBUG] BenchmarkTracker.__init__: loaded {len(self.benchmark_history)} history items, {len(self.step_summaries)} summaries")
        print(f"[DEBUG] BenchmarkTracker.__init__: question_cache_size={len(self._question_history)}")
        
        # Store config for later use
        self.config = config
    
    def _load_history(self) -> List[BenchmarkResult]:
        """Load benchmark history from disk."""
        if self.history_file.exists():
            try:
                with open(self.history_file, 'rb') as f:
                    data = pickle.load(f)
                    print(f"[DEBUG] BenchmarkTracker._load_history: Loaded {len(data)} items")
                    return data
            except Exception as e:
                print(f"[DEBUG] BenchmarkTracker._load_history: Error loading history: {e}")
                PrettyPrinter.status("TRACKER", f"Error loading history: {e}", "warn")
        print(f"[DEBUG] BenchmarkTracker._load_history: No existing history file")
        return []
    
    def _load_summaries(self) -> List[StepSummary]:
        """Load step summaries from disk."""
        if self.summaries_file.exists():
            try:
                with open(self.summaries_file, 'rb') as f:
                    data = pickle.load(f)
                    print(f"[DEBUG] BenchmarkTracker._load_summaries: Loaded {len(data)} summaries")
                    return data
            except Exception as e:
                print(f"[DEBUG] BenchmarkTracker._load_summaries: Error loading summaries: {e}")
                PrettyPrinter.status("TRACKER", f"Error loading summaries: {e}", "warn")
        print(f"[DEBUG] BenchmarkTracker._load_summaries: No existing summaries file")
        return []
    
    def _save_history(self):
        """Save benchmark history to disk."""
        try:
            with open(self.history_file, 'wb') as f:
                pickle.dump(self.benchmark_history, f)
            print(f"[DEBUG] BenchmarkTracker._save_history: Saved {len(self.benchmark_history)} items")
        except Exception as e:
            print(f"[DEBUG] BenchmarkTracker._save_history: Error saving history: {e}")
            PrettyPrinter.status("TRACKER", f"Error saving history: {e}", "error")
    
    def _save_summaries(self):
        """Save step summaries to disk."""
        try:
            with open(self.summaries_file, 'wb') as f:
                pickle.dump(self.step_summaries, f)
            print(f"[DEBUG] BenchmarkTracker._save_summaries: Saved {len(self.step_summaries)} summaries")
        except Exception as e:
            print(f"[DEBUG] BenchmarkTracker._save_summaries: Error saving summaries: {e}")
            PrettyPrinter.status("TRACKER", f"Error saving summaries: {e}", "error")
    
    def _rebuild_question_cache(self):
        """Rebuild the question-based cache from history."""
        self._question_history.clear()
        for result in self.benchmark_history:
            self._question_history[result.question_id].append(result)
        
        # Sort by step for each question
        for question_id in self._question_history:
            self._question_history[question_id].sort(key=lambda x: x.step)
    
    def record_benchmark_results(
        self, 
        results: List[Dict[str, Any]], 
        step: int,
        benchmark_name: str = "general"
    ):
        """Record benchmark results for a specific step."""
        timestamp = datetime.now().isoformat()
        
        # Convert results to BenchmarkResult objects
        benchmark_results = []
        for result in results:
            benchmark_result = BenchmarkResult(
                question_id=result.get('question_id', f"{benchmark_name}_{len(benchmark_results)}"),
                question=result.get('question', ''),
                model_answer=result.get('model_answer', ''),
                correct_answer=result.get('correct_answer', ''),
                is_correct=result.get('is_correct', False),
                score=result.get('score', 0.0),
                step=step,
                timestamp=timestamp,
                benchmark_name=benchmark_name
            )
            benchmark_results.append(benchmark_result)
        
        # Add to history
        self.benchmark_history.extend(benchmark_results)
        
        # Update question cache
        for result in benchmark_results:
            # Debug: Show question ID and existing history length
            existing_count = len(self._question_history[result.question_id])
            print(f"[DEBUG] Adding result for question_id={result.question_id}, existing_history_len={existing_count}, step={result.step}")
            
            self._question_history[result.question_id].append(result)
            self._question_history[result.question_id].sort(key=lambda x: x.step)
            
            # Debug: Show new history length
            new_count = len(self._question_history[result.question_id])
            print(f"[DEBUG] After adding: question_id={result.question_id}, new_history_len={new_count}")
            
            # Show first few characters of question for identification
            question_preview = result.question[:50] + "..." if len(result.question) > 50 else result.question
            print(f"[DEBUG] Question preview: {question_preview}")
        
        # Create step summary
        self._create_step_summary(step, timestamp, benchmark_results)
        
        # Save to disk
        self._save_history()
        self._save_summaries()
        
        PrettyPrinter.status("TRACKER", f"Recorded {len(benchmark_results)} results for step {step}", "success")
    
    def _create_step_summary(self, step: int, timestamp: str, results: List[BenchmarkResult]):
        """Create or update summary for a specific step."""
        print(f"[DEBUG] _create_step_summary: Creating summary for step {step}")
        if not results:
            return
        
        # Check if we already have a summary for this step
        existing_summary = None
        existing_index = None
        for i, summary in enumerate(self.step_summaries):
            if summary.step == step:
                existing_summary = summary
                existing_index = i
                print(f"[DEBUG] _create_step_summary: Found existing summary for step {step}")
                break
        
        if existing_summary:
            # Update existing summary by merging with new results
            print(f"[DEBUG] _create_step_summary: Updating existing summary for step {step}")
            
            # Get all results for this step from history
            all_step_results = [r for r in self.benchmark_history if r.step == step]
            print(f"[DEBUG] _create_step_summary: Found {len(all_step_results)} total results for step {step}")
            
            # Recalculate overall accuracy
            correct = sum(1 for r in all_step_results if r.is_correct)
            total = len(all_step_results)
            overall_accuracy = correct / total if total > 0 else 0.0
            
            # Recalculate per-benchmark accuracy
            benchmark_accuracies = defaultdict(lambda: {'correct': 0, 'total': 0})
            for result in all_step_results:
                benchmark_accuracies[result.benchmark_name]['total'] += 1
                if result.is_correct:
                    benchmark_accuracies[result.benchmark_name]['correct'] += 1
            
            # Convert to final format
            final_accuracies = {}
            for benchmark, stats in benchmark_accuracies.items():
                final_accuracies[benchmark] = stats['correct'] / stats['total'] if stats['total'] > 0 else 0.0
            
            # Update the existing summary
            self.step_summaries[existing_index] = StepSummary(
                step=step,
                timestamp=timestamp,  # Use latest timestamp
                overall_accuracy=overall_accuracy,
                benchmark_accuracies=final_accuracies,
                total_questions=total,
                correct_questions=correct
            )
            
            print(f"[DEBUG] _create_step_summary: Updated summary for step {step}")
            print(f"[DEBUG] _create_step_summary: Step {step} now has {len(final_accuracies)} benchmarks: {list(final_accuracies.keys())}")
            
        else:
            # Create new summary
            print(f"[DEBUG] _create_step_summary: Creating new summary for step {step}")
            
            # Overall accuracy
            correct = sum(1 for r in results if r.is_correct)
            total = len(results)
            overall_accuracy = correct / total if total > 0 else 0.0
            
            # Per-benchmark accuracy
            benchmark_accuracies = defaultdict(lambda: {'correct': 0, 'total': 0})
            for result in results:
                benchmark_accuracies[result.benchmark_name]['total'] += 1
                if result.is_correct:
                    benchmark_accuracies[result.benchmark_name]['correct'] += 1
            
            # Convert to final format
            final_accuracies = {}
            for benchmark, stats in benchmark_accuracies.items():
                final_accuracies[benchmark] = stats['correct'] / stats['total'] if stats['total'] > 0 else 0.0
            
            summary = StepSummary(
                step=step,
                timestamp=timestamp,
                overall_accuracy=overall_accuracy,
                benchmark_accuracies=final_accuracies,
                total_questions=total,
                correct_questions=correct
            )
            
            self.step_summaries.append(summary)
            print(f"[DEBUG] _create_step_summary: Added new summary for step {step}, total summaries now: {len(self.step_summaries)}")
        
        # Debug current step's benchmarks
        current_summary = None
        for s in self.step_summaries:
            if s.step == step:
                current_summary = s
                break
                
        if current_summary:
            print(f"[DEBUG] _create_step_summary: Step {step} final benchmarks: {list(current_summary.benchmark_accuracies.keys())}")
            for bench_name, accuracy in current_summary.benchmark_accuracies.items():
                print(f"[DEBUG] _create_step_summary:   {bench_name}: {accuracy:.3f}")
        
        print(f"[DEBUG] _create_step_summary: All steps in summaries: {[s.step for s in self.step_summaries]}")
    
    def analyze_performance_trends(self, min_steps: int = 2) -> Dict[str, Any]:
        """Analyze performance trends over time."""
        print(f"[DEBUG] analyze_performance_trends: step_summaries={len(self.step_summaries)}")
        
        if len(self.step_summaries) < min_steps:
            return {"message": f"Need at least {min_steps} validation steps for trend analysis"}
        
        # Sort summaries by step
        sorted_summaries = sorted(self.step_summaries, key=lambda x: x.step)
        print(f"[DEBUG] analyze_performance_trends: sorted steps={[s.step for s in sorted_summaries]}")
        
        # Overall trend
        accuracies = [s.overall_accuracy for s in sorted_summaries]
        steps = [s.step for s in sorted_summaries]
        
        # Calculate trend
        if len(accuracies) >= 2:
            recent_accuracy = accuracies[-1]
            previous_accuracy = accuracies[-2]
            accuracy_change = recent_accuracy - previous_accuracy
        else:
            accuracy_change = 0.0
        
        # Per-benchmark trends
        benchmark_trends = {}
        all_benchmarks = set()
        for summary in sorted_summaries:
            all_benchmarks.update(summary.benchmark_accuracies.keys())
            
        print(f"[DEBUG] analyze_performance_trends: all_benchmarks={all_benchmarks}")
        print(f"[DEBUG] analyze_performance_trends: sample benchmark_accuracies={[s.benchmark_accuracies for s in sorted_summaries[:2]]}")
        
        for benchmark in all_benchmarks:
            benchmark_scores = []
            for summary in sorted_summaries:
                if benchmark in summary.benchmark_accuracies:
                    benchmark_scores.append(summary.benchmark_accuracies[benchmark])
            
            if len(benchmark_scores) >= 2:
                trend = benchmark_scores[-1] - benchmark_scores[-2]
                benchmark_trends[benchmark] = {
                    'current_accuracy': benchmark_scores[-1],
                    'previous_accuracy': benchmark_scores[-2], 
                    'trend': trend,
                    'history': benchmark_scores
                }
        
        return {
            'overall_accuracy_change': accuracy_change,
            'current_overall_accuracy': accuracies[-1] if accuracies else 0.0,
            'benchmark_trends': benchmark_trends,
            'total_steps_analyzed': len(sorted_summaries),
            'steps': steps,
            'accuracies': accuracies
        }
    
    def find_problematic_questions(self, window_size: int = 5) -> Dict[str, List[str]]:
        """Find questions that are consistently wrong or got worse."""
        
        print(f"[DEBUG] find_problematic_questions called: step_summaries={len(self.step_summaries)}, question_history_size={len(self._question_history)}")
        print(f"[DEBUG] Total benchmark_history items: {len(self.benchmark_history)}")
        
        # Debug: Show what steps we actually have
        all_summary_steps = [s.step for s in self.step_summaries]
        print(f"[DEBUG] All step_summary.step values: {all_summary_steps}")
        print(f"[DEBUG] Unique step values: {sorted(list(set(all_summary_steps)))}")
        
        if len(self.step_summaries) < 2:
            return {"message": "Need at least 2 validation steps for question analysis"}
        
        all_steps = sorted(list(set([s.step for s in self.step_summaries])))
        recent_steps = all_steps  # Use ALL unique steps if we have duplicates
        
        print(f"[DEBUG] All available steps: {all_steps}")
        print(f"[DEBUG] Window size: {window_size}")
        print(f"[DEBUG] Recent steps to analyze: {recent_steps}")
        print(f"[DEBUG] Type of recent_steps: {type(recent_steps)}, contents: {recent_steps}")
        
        problematic_questions = {
            'always_wrong': [],  # Questions always answered incorrectly
            'got_worse': [],     # Questions that were correct before but wrong now
            'inconsistent': []   # Questions with inconsistent performance
        }
        
        analyzed_questions = 0
        skipped_single_occurrence = 0
        
        # Add debugging to see what data we have
        print(f"[DEBUG] Question cache has {len(self._question_history)} unique questions")
        print(f"[DEBUG] Recent steps to analyze: {recent_steps}")
        
        # Show a sample of the question history
        sample_count = 0
        for question_id, history in self._question_history.items():
            if sample_count < 3:  # Show first 3 questions
                print(f"[DEBUG] Sample question {question_id}: {len(history)} history items")
                for h in history:
                    print(f"[DEBUG]   - Step {h.step}, correct: {h.is_correct}, benchmark: {h.benchmark_name}")
                sample_count += 1
        
        for question_id, history in self._question_history.items():
            # Skip questions that only appeared once (can't determine pattern)
            if len(history) < 2:
                skipped_single_occurrence += 1
                continue
            
            # Filter to recent steps
            recent_history = [h for h in history if h.step in recent_steps]
            
            # Debug the filtering process
            if analyzed_questions < 3:  # Debug first 3 questions
                steps_in_history = [h.step for h in history]
                print(f"[DEBUG] Question {question_id}: steps_in_history={steps_in_history}, recent_steps={recent_steps}")
                print(f"[DEBUG] Filtered recent_history: {[h.step for h in recent_history]} (length={len(recent_history)})")
            
            # Need at least 2 data points to analyze patterns
            if len(recent_history) < 2:
                if analyzed_questions < 3:
                    print(f"[DEBUG] Question {question_id}: Skipped - only {len(recent_history)} recent data points")
                continue
            
            analyzed_questions += 1
            if analyzed_questions <= 5:  # Debug first 5 questions
                print(f"[DEBUG] Question {question_id}: total_history_len={len(history)}, recent_history_len={len(recent_history)}")
                print(f"[DEBUG] Recent results: {[(h.step, h.is_correct) for h in recent_history]}")
                print(f"[DEBUG] Full history: {[(h.step, h.is_correct) for h in history]}")
            
            # Analyze pattern
            recent_correct = [h.is_correct for h in recent_history]
            all_correct = [h.is_correct for h in history]
            
            # Always wrong in recent history (and historically)
            if not any(recent_correct):
                # Also check if it was never correct in the entire history
                if not any(all_correct):
                    problematic_questions['always_wrong'].append(question_id)
                    if analyzed_questions <= 5:
                        print(f"[DEBUG] Added {question_id} to always_wrong (never correct in {len(history)} attempts)")
            
            # Got worse (was correct before, wrong now)
            elif len(recent_correct) >= 2 and recent_correct[0] and not recent_correct[-1]:
                problematic_questions['got_worse'].append(question_id)
                if analyzed_questions <= 5:
                    print(f"[DEBUG] Added {question_id} to got_worse")
            
            # Alternative check for "got worse": was correct in earlier history but wrong in recent
            elif any(all_correct[:len(all_correct)//2]) and not any(recent_correct):
                problematic_questions['got_worse'].append(question_id)
                if analyzed_questions <= 5:
                    print(f"[DEBUG] Added {question_id} to got_worse (was correct earlier, now consistently wrong)")
            
            # Inconsistent (alternating correct/incorrect)
            elif len(set(recent_correct)) > 1 and len(recent_correct) > 2:
                # Check if it's truly inconsistent (not just improved)
                changes = sum(1 for i in range(1, len(recent_correct)) 
                             if recent_correct[i] != recent_correct[i-1])
                if changes >= 2:
                    problematic_questions['inconsistent'].append(question_id)
                    if analyzed_questions <= 5:
                        print(f"[DEBUG] Added {question_id} to inconsistent ({changes} changes in recent history)")
        
        print(f"[DEBUG] find_problematic_questions results:")
        print(f"[DEBUG] - always_wrong: {len(problematic_questions['always_wrong'])} questions")
        print(f"[DEBUG] - got_worse: {len(problematic_questions['got_worse'])} questions") 
        print(f"[DEBUG] - inconsistent: {len(problematic_questions['inconsistent'])} questions")
        print(f"[DEBUG] - total analyzed questions: {analyzed_questions}")
        print(f"[DEBUG] - skipped single occurrence questions: {skipped_single_occurrence}")
        
        return problematic_questions
    
    def generate_improvement_prompt(self) -> str:
        """Generate a prompt suggesting improvements based on performance analysis."""
        print(f"[DEBUG] generate_improvement_prompt: step_summaries={len(self.step_summaries)}")
        
        if len(self.step_summaries) < 2:
            print(f"[DEBUG] generate_improvement_prompt: Not enough validation data")
            return "Not enough validation data to generate improvement suggestions."
        
        trends = self.analyze_performance_trends()
        problematic_questions = self.find_problematic_questions()
        
        print(f"[DEBUG] generate_improvement_prompt: trends={trends.get('overall_accuracy_change', 'N/A')}")
        print(f"[DEBUG] generate_improvement_prompt: problematic_questions keys={list(problematic_questions.keys())}")
        
        prompt_parts = []
        
        # Overall performance summary
        current_acc = trends['current_overall_accuracy'] * 100
        change = trends.get('overall_accuracy_change', 0) * 100
        
        if change > 0:
            trend_desc = f"improved by {change:.1f}%"
        elif change < 0:
            trend_desc = f"declined by {abs(change):.1f}%"
        else:
            trend_desc = "remained stable"
        
        prompt_parts.append(f"## Performance Analysis Summary\n")
        prompt_parts.append(f"Current overall accuracy: {current_acc:.1f}% (has {trend_desc})\n")
        
        # Per-benchmark analysis
        if trends.get('benchmark_trends'):
            prompt_parts.append("\n## Benchmark-Specific Trends:")
            for benchmark, trend_data in trends['benchmark_trends'].items():
                current = trend_data['current_accuracy'] * 100
                change = trend_data.get('trend', 0) * 100
                if change > 0:
                    change_desc = f"↑{change:.1f}%"
                elif change < 0:
                    change_desc = f"↓{abs(change):.1f}%"
                else:
                    change_desc = "stable"
                prompt_parts.append(f"- {benchmark}: {current:.1f}% ({change_desc})")
                
                # Show history for better context
                if 'history' in trend_data and len(trend_data['history']) > 1:
                    history_str = " → ".join([f"{acc:.1f}%" for acc in [h*100 for h in trend_data['history']]])
                    prompt_parts.append(f"  History: {history_str}")
        else:
            print(f"[DEBUG] No benchmark trends available")
        
        # Problematic questions analysis
        problem_categories = ['always_wrong', 'got_worse', 'inconsistent']
        has_problems = any(problematic_questions.get(k, []) for k in problem_categories)
        
        print(f"[DEBUG] generate_improvement_prompt: has_problems={has_problems}")
        for category in problem_categories:
            count = len(problematic_questions.get(category, []))
            print(f"[DEBUG] generate_improvement_prompt: {category}: {count} questions")
        
        if has_problems:
            prompt_parts.append(f"\n## Problem Areas Identified:")
            
            if problematic_questions.get('always_wrong'):
                count = len(problematic_questions['always_wrong'])
                prompt_parts.append(f"- {count} questions consistently answered incorrectly")
                print(f"[DEBUG] Added always_wrong section: {count} questions")
                
                # Show examples of always wrong questions
                if count > 0:
                    prompt_parts.append(f"\n### Examples of Always Wrong Questions:")
                    examples = problematic_questions['always_wrong'][:3]  # Show first 3
                    for i, question_id in enumerate(examples, 1):
                        if question_id in self._question_history:
                            latest = self._question_history[question_id][-1]
                            question_preview = latest.question
                            prompt_parts.append(f"{i}. **{question_id}**: {question_preview}")
                            prompt_parts.append(f"   Model Answer: {latest.model_answer}...")
                            prompt_parts.append(f"   Correct Answer: {latest.correct_answer}...")
                            prompt_parts.append("")
            
            if problematic_questions.get('got_worse'):
                count = len(problematic_questions['got_worse'])
                prompt_parts.append(f"- {count} questions that were correct before but are now wrong")
                print(f"[DEBUG] Added got_worse section: {count} questions")
                
                # Show examples of got worse questions
                if count > 0:
                    prompt_parts.append(f"\n### Examples of Regressed Questions:")
                    examples = problematic_questions['got_worse'][:2]  # Show first 2
                    for i, question_id in enumerate(examples, 1):
                        if question_id in self._question_history:
                            history = self._question_history[question_id]
                            latest = history[-1]
                            question_preview = latest.question[:100] + "..." if len(latest.question) > 100 else latest.question
                            performance = " → ".join([str(h.is_correct) for h in history])
                            prompt_parts.append(f"{i}. **{question_id}**: {question_preview}")
                            prompt_parts.append(f"   Performance: {performance}")
                            prompt_parts.append("")
            
            if problematic_questions.get('inconsistent'):
                count = len(problematic_questions['inconsistent'])
                prompt_parts.append(f"- {count} questions with inconsistent performance")
                print(f"[DEBUG] Added inconsistent section: {count} questions")
        else:
            print(f"[DEBUG] No problem areas identified")
        
        # Improvement suggestions
        prompt_parts.append(f"\n## Suggested Improvements:")
        
        if trends.get('overall_accuracy_change', 0) < -0.05:  # Significant decline
            prompt_parts.append("- **URGENT**: Overall performance has declined significantly. Consider:")
            prompt_parts.append("  - Reviewing recent changes to judge, proposer, or solver prompts")
            prompt_parts.append("  - Checking if the model is overfitting to recent training data")
            prompt_parts.append("  - Reverting to previous prompt versions if available")
        
        if problematic_questions.get('always_wrong'):
            prompt_parts.append("- **Judge Prompt**: Consider improving the evaluation criteria for consistently failed questions")
            prompt_parts.append("- **Solver Prompt**: Add specific instructions for handling the types of problems that are always wrong")
        
        if problematic_questions.get('got_worse'):
            prompt_parts.append("- **Proposer Prompt**: The question generation may be drifting from effective patterns")
            prompt_parts.append("- **Training Stability**: Check if the model is forgetting previously learned skills")
        
        if problematic_questions.get('inconsistent'):
            prompt_parts.append("- **Consistency**: Add instructions for more consistent reasoning approaches")
            prompt_parts.append("- **Judge Prompt**: Consider more robust evaluation criteria to reduce scoring variance")
        
        # Specific recommendations based on trend direction
        if trends.get('overall_accuracy_change', 0) > 0.02:
            prompt_parts.append("- **Positive Trend**: Current approach is working well, consider reinforcing successful patterns")
        
        final_prompt = "\n".join(prompt_parts)
        print(f"[DEBUG] generate_improvement_prompt: Generated prompt length={len(final_prompt)}")
        return final_prompt
    
    def get_question_details(self, question_ids: List[str]) -> List[Dict]:
        """Get detailed history for specific questions."""
        details = []
        for question_id in question_ids:
            if question_id in self._question_history:
                history = self._question_history[question_id]
                latest = history[-1]  # Most recent result
                
                # Performance over time
                performance = [h.is_correct for h in history]
                
                details.append({
                    'question_id': question_id,
                    'question': latest.question,
                    'latest_answer': latest.model_answer,
                    'correct_answer': latest.correct_answer,
                    'performance_history': performance,
                    'steps': [h.step for h in history],
                    'latest_step': latest.step
                })
        
        return details
    
    def export_analysis_report(self, output_file: Optional[Path] = None) -> str:
        """Export a comprehensive analysis report."""
        if output_file is None:
            output_file = self.output_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("BENCHMARK PERFORMANCE ANALYSIS REPORT")
        report_lines.append("=" * 80)
        report_lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"Total validation steps: {len(self.step_summaries)}")
        report_lines.append("")
        
        # Add improvement prompt
        report_lines.append(self.generate_improvement_prompt())
        report_lines.append("")
        
        # Detailed trend analysis
        if len(self.step_summaries) >= 2:
            trends = self.analyze_performance_trends()
            report_lines.append("## Detailed Trend Data:")
            report_lines.append(f"Steps analyzed: {trends['steps']}")
            report_lines.append(f"Accuracy progression: {[f'{acc:.3f}' for acc in trends['accuracies']]}")
        
        report_content = "\n".join(report_lines)
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            PrettyPrinter.status("TRACKER", f"Analysis report exported to {output_file}", "success")
        except Exception as e:
            PrettyPrinter.status("TRACKER", f"Error exporting report: {e}", "error")
        
        return report_content
    
    def record_validation_results(
        self, 
        step: int,
        benchmark_name: str,
        questions: List[str],
        model_answers: List[str],
        ground_truths: List[str],
        scores: List[float],
        accuracy: float
    ):
        """Record validation results for a benchmark at a specific step.
        
        This is the interface called by the trainer during validation.
        """
        print(f"[DEBUG] BenchmarkTracker.record_validation_results: step={step}, benchmark={benchmark_name}, questions={len(questions)}")
        print(f"[DEBUG] BenchmarkTracker.record_validation_results: accuracy={accuracy:.3f}")
        
        # Convert to the format expected by record_benchmark_results
        results = []
        for i, (question, model_answer, ground_truth, score) in enumerate(
            zip(questions, model_answers, ground_truths, scores)
        ):
            # Create a stable question_id based on question content and benchmark
            # Use a hash of the question text to ensure same question has same ID across steps
            import hashlib
            question_hash = hashlib.md5(f"{benchmark_name}:{question}".encode('utf-8')).hexdigest()[:8]
            stable_question_id = f"{benchmark_name}_{question_hash}"
            
            result = {
                'question_id': stable_question_id,  # Use stable ID instead of step-dependent ID
                'question': question,
                'model_answer': model_answer,
                'correct_answer': ground_truth,
                'is_correct': score > 0.5,  # Assuming threshold of 0.5
                'score': score
            }
            results.append(result)
        
        print(f"[DEBUG] BenchmarkTracker.record_validation_results: Created {len(results)} result objects with stable IDs")
        
        # Record using the existing method
        self.record_benchmark_results(results, step, benchmark_name)
        
        print(f"[DEBUG] BenchmarkTracker.record_validation_results: Recording completed")
    
    def generate_analysis_report(self, step: int) -> Optional[str]:
        """Generate an analysis report for the current step."""
        print(f"[DEBUG] BenchmarkTracker.generate_analysis_report: step={step}, history_length={len(self.benchmark_history)}")
        
        if len(self.step_summaries) < 2:
            print(f"[DEBUG] BenchmarkTracker.generate_analysis_report: Not enough data for analysis (need >=2 steps)")
            return None
            
        try:
            report = self.export_analysis_report()
            print(f"[DEBUG] BenchmarkTracker.generate_analysis_report: Generated report of length {len(report)}")
            return report
        except Exception as e:
            print(f"[DEBUG] BenchmarkTracker.generate_analysis_report: Error generating report: {e}")
            return None
    
    def generate_improvement_prompts(self, step: int) -> Dict[str, str]:
        """Generate improvement prompts based on current performance."""
        print(f"[DEBUG] BenchmarkTracker.generate_improvement_prompts: step={step}")
        
        if len(self.step_summaries) < 2:
            print(f"[DEBUG] BenchmarkTracker.generate_improvement_prompts: Not enough data for prompts")
            return {}
        
        try:
            # Find problematic questions
            problematic = self.find_problematic_questions()
            print(f"[DEBUG] BenchmarkTracker.generate_improvement_prompts: Found problematic questions: {list(problematic.keys())}")
            
            # Generate improvement prompt
            base_prompt = self.generate_improvement_prompt()
            
            prompts = {
                'judge': f"Judge Prompt Improvement:\n{base_prompt}",
                'proposer': f"Proposer Prompt Improvement:\n{base_prompt}",
                'solver': f"Solver Prompt Improvement:\n{base_prompt}"
            }
            
            print(f"[DEBUG] BenchmarkTracker.generate_improvement_prompts: Generated {len(prompts)} prompts")
            return prompts
            
        except Exception as e:
            print(f"[DEBUG] BenchmarkTracker.generate_improvement_prompts: Error: {e}")
            return {}
    
    def save_analysis_to_files(self, step: int, analysis_report: Optional[str], improvement_prompts: Dict[str, str]):
        """Save analysis and prompts to files."""
        print(f"[DEBUG] BenchmarkTracker.save_analysis_to_files: step={step}")
        
        try:
            # Save analysis report
            if analysis_report:
                analysis_file = self.output_dir / f"analysis_step_{step}.txt"
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    f.write(analysis_report)
                print(f"[DEBUG] BenchmarkTracker.save_analysis_to_files: Saved analysis to {analysis_file}")
            
            # Save improvement prompts
            if improvement_prompts:
                prompts_file = self.output_dir / f"improvement_prompts_step_{step}.json"
                with open(prompts_file, 'w', encoding='utf-8') as f:
                    json.dump(improvement_prompts, f, indent=2, ensure_ascii=False)
                print(f"[DEBUG] BenchmarkTracker.save_analysis_to_files: Saved prompts to {prompts_file}")
                
            print(f"[DEBUG] BenchmarkTracker.save_analysis_to_files: All files saved successfully")
            
        except Exception as e:
            print(f"[DEBUG] BenchmarkTracker.save_analysis_to_files: Error saving files: {e}")
            import traceback
            traceback.print_exc()
